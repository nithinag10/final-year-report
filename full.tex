\documentclass[ 12pt,a4paper,twocolumn,fleqn]{article}
\usepackage{graphicx}
\usepackage[a4paper,top=20mm, bottom=30mm, left=10mm, right=25mm]{geometry}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{listings}
\usepackage[]{algorithm2e}
\usepackage{color}
\usepackage{fancybox}
\thisfancypage{%
  \setlength{\fboxsep}{20pt}\doublebox}{}
\pagestyle{fancy}
\usepackage{lineno}
\usepackage{xtab,booktabs}
\usepackage{setspace}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage[compact]{titlesec}
\titlespacing{\section}{0pt}{*0}{*0}
\titlespacing{\subsection}{0pt}{*0}{*0}
\titlespacing{\subsubsection}{0pt}{*0}{*0}
\setlength\columnsep{25pt}
\makeatletter
\g@addto@macro{\normalsize}{%
\setlength{\abovedisplayskip}{0pt}
\setlength{\abovedisplayshortskip}{0pt}
\setlength{\belowdisplayskip}{0pt}
\setlength{\belowdisplayshortskip}{0pt}}
\makeatother
\mathindent=0.0pt
\usepackage{float}
\renewcommand{\baselinestretch}{1.5}
\begin{document}
\onecolumn
\begin{center}
\text{A Project Report On} \\
\smallskip
\textcolor{red}{\LARGE{Adversarial Attack on Autonomous Vehicles}} \\
\large{Submitted in partial fulfillment of the requirement for the $8^{th}$ semester}
\large{\textbf{Bachelor of Engineering}} \\
\large{in} \\
\large{Computer Science and Engineering} \\
\textcolor{blue}{\LARGE{DAYANANDA SAGAR COLLEGE OF ENGINEERING}} \\
\footnotesize{(An Autonomous Institute affiliated to VTU, Belagavi, Approved by AICTE \& ISO 9001:2008 Certified)} \\
\footnotesize{Accredited by National Assessment \& Accreditation Council (NAAC) with ‘A’ grade}  \\
\footnotesize{Shavige Malleshwara Hills, Kumaraswamy Layout, Bengaluru-560078} \\
\includegraphics[scale=0.4]{media/DSCE-min.png} \\
\textit{Submitted By} \\
\textbf{Manoj B Bhamsagar \space 1DS18CS066} \\
\textbf{Nithin A G \space 1DS18CS081} \\
\textbf{Prajwal Ponnana \space 1DS18CS093} \\
\textbf{Pururav H K \space 1DS18CS098} \\
\textit{Under the guidance of} \\
\textbf{Prof. Sarala D V}\\
\text{Asst Professor, CSE , DSCE}\\
\Large{\textbf{2020 - 2021}} \\
\textcolor{blue}{\Large{Department of Computer Science and Engineering}} \\
\textcolor{blue}{\Large{DAYANANDA SAGAR COLLEGE OF ENGINEERING}} \\
\textcolor{blue}{\Large{Bangalore - 560078}} \\
\end{center}
\newpage
  \pagestyle{fancy}
\thisfancypage{%
  \setlength{\fboxsep}{20pt}\doublebox}{}
\begin{center}
\textcolor{red}{\LARGE{VISVESVARAYA TECHNOLOGICAL UNIVERSITY}} \\
\textcolor{red}{\LARGE{Dayananda Sagar College of Engineering}} \\
\footnotesize{(An Autonomous Institute affiliated to VTU, Belagavi, Approved by AICTE \& ISO 9001:2008 Certified)} \\
\footnotesize{Accredited by National Assessment \& Accreditation Council (NAAC) with ‘A’ grade}  \\
\footnotesize{Shavige Malleshwara Hills, Kumaraswamy Layout, Bengaluru-560078} \\
\begin{flushleft}
\textcolor{blue}{\LARGE{\textbf{Department of Computer Science \& Engineering}}} \\
\end{flushleft}
\includegraphics[scale=0.4]{media/DSCE-min.png} \\
\Large{\underline{\textbf{CERTIFICATE}}} \\
  \end{center}
\normalsize
This is to certify that the project entitled \textbf{Adversarial Attack on Autonomous Vehicles} is a bonafide work carried out by \textbf{Manoj B Bhamsagar, [1DS18CS066]}, \textbf{Nithin A G [1DS18CS081], Prajwal Ponnana [1DS18CS093]} and \textbf{Pururav H K [1DS18CS098]} in partial fulfillment of 8th semester, Bachelor of Engineering in Computer Science and Engineering under Visvesvaraya Technological University, Belgaum during the year 2020-21. \\
\\
\textbf{Prof. Sarala D V}
\hfill
\textbf{Dr. Ramesh Babu D R}
\hfill
\textbf{Dr. C P S Prakash} \\
\text{(Internal Guide)}
\hfill
\text{Vice Principal \& HOD}
\hfill
\text{Principal} \\
\text{Asst Prof. CSE, DSCE} 
\hfill
\text{CSE, DSCE}
\hfill
\text{DSCE} \\
\\
\text{Signature:...........}
\hfill
\text{Signature:...........}
\hfill
\text{Signature:...........} \\
\\
\\
\text{Name of the Examiners:}
\hfill
\text{Signature with date:} \\
\text{1...........................}
\hfill{.............................} \\
\text{2.............................} 
\hfill{............................} \\
\newpage
  \pagestyle{fancy}
\thisfancypage{%
  \setlength{\fboxsep}{20pt}\doublebox}{}
\begin{center}
\LARGE{{\underline{Acknowledgement}}} \\
\end{center}
\normalsize
We are pleased to have successfully completed the project \textbf{Adversarial Attack on Autonomous Vehicles}. We thoroughly enjoyed the process of working on this project and gained a lot of knowledge doing so.
\\
\hfill
\\
We would like to take this opportunity to express our gratitude to \textbf{Dr. C P S Prakash}, Principal of DSCE, for permitting us to utilize all the necessary facilities of the institution.
\\
\hfill
\\
We also thank our respected Vice Principal, HOD of Computer Science \& Engineering, DSCE, Bangalore,\textbf{ Dr. Ramesh Babu D R}, for his support and encouragement throughout the process.
\\
\hfill
\\
We are immensely grateful to our respected and learned guide, \textbf{Sarala D V} Assistant Professor CSE, DSCE for their valuable help and guidance. We are indebted to them for their invaluable guidance throughout the process and their useful inputs at all stages of the process.
\\
\hfill
\\
We also thank all the faculty and support staff of Department of Computer Science, DSCE. Without their support over the years, this work would not have been possible.
\\
\hfill
\\
Lastly, we would like to express our deep appreciation towards our classmates and our family for providing us with constant moral support and encouragement. They have stood by us in the most difficult of times.
\\
\hfill
\\
\begin{flushright}
\textbf{Manoj B Bhamsagar \space 1DS18CS066} \\
\textbf{Nithin A G \space 1DS18CS081} \\
\textbf{Prajwal Ponnana \space 1DS18CS093} \\
\textbf{Pururav H K \space 1DS18CS098} \\
\end{flushright}
\newpage
  \pagestyle{fancy}
\thisfancypage{%
  \setlength{\fboxsep}{20pt}\doublebox}{}
\setstretch{1.5}
\twocolumn[
\begin{@twocolumnfalse}
\title {Adversarial Attack on Autonomous Vehicles}
\author{Manoj, Nithin, Prajwal, Pururav}
\maketitle
\begin{abstract}
Deep learning is at the core of the present emergence of artificial intelligence. In the realm of computer vision, it has become the workhorse for applications ranging from self-driving cars to surveillance and security. While deep neural networks have exhibited extraordinary effectiveness (often exceeding human capabilities) in addressing difficult problems, new research shows that they are subject to adversarial assaults in the form of minor changes to inputs that cause a model to anticipate wrong outputs. Such disturbances are generally too subtle to be seen in photos, but they fully deceive deep learning networks. Adversarial assaults are a severe danger to the implementation of deep learning. This has lately led to a significant increase in donations in this direction. \\
%
Adversarial machine learning is a machine learning strategy that uses false input to fool machine learning models. As a result, it comprises both the production and detection of adversarial instances, which are inputs designed specifically to fool classifiers. Adversarial machine learning attacks have been intensively researched in fields such as image classification and spam detection.\\
%
Our project involves the development of two white-box targeted attacks against end-to-end autonomous driving systems. The driving model receives an image and outputs the steering angle. Only by changing the input image can our attacks influence the behavior of the autonomous driving system. On CPUs, both attacks can be launched in real time. This demonstration intends to raise concerns about the use of end-to-end models in safety-critical systems.
\end{abstract}
\end{@twocolumnfalse}]
\linenumbers
\modulolinenumbers[5]
\onecolumn
\newpage
  \pagestyle{fancy}
\thisfancypage{%
  \setlength{\fboxsep}{20pt}\doublebox}{}
\tableofcontents
\newpage
  \pagestyle{fancy}f
\thisfancypage{%
  \setlength{\fboxsep}{20pt}\doublebox}{}
\part{Overview}
Welcome to the main body of the report. As you can see we have used the part tag above to label the first part of our report, called the overview. We have done this so that \LaTeX{} can automatically number our document. There are multiple tiers to this numbering, such as sections, subsections, subsubsections and paragraphs. You can also use chapters, but for that at the top of the document you must enable report instead of article. I think 4 levels of nesting is enough. \LaTeX{} also uses the information you have given it to populate the table of contents (Index). So please use the appropriate tags instead of manually changing the font, it will make your document easier to edit, maintain and will also give you a neatly formatted index.
\section{Introduction}
This is a section. I have used the section tab which you can view in the source file.
\subsection{Autonomous Vehicles}
Autonomous vehicles (AVs) have the potential to be both disruptive and useful to our transportation system. This innovative technology has the potential to have an impact on vehicle safety, traffic congestion, and travel behavior. Overall, important societal AV impacts in the form of collision saves, reduced travel time, improved fuel economy, and parking advantages are expected to be around 2000 dollars per AV per year, and may eventually reach approximately 4000 dollars when all crash costs are considered.\\
%
Many companies are actively developing related hardware and software technologies toward fully autonomous driving capability with no human intervention, which has sparked increased interest in autonomous cars in recent years. Deep neural networks (DNNs) have recently been used successfully in a variety of perception and control tasks. They are also significant workloads for autonomous vehicles.\\
%
The Tesla Model S, for example, was known to use a specialized chip (MobileEye EyeQ) with a vision-based real-time obstacle detection system based on a DNN. Researchers are currently investigating DNN-based end-to-end real-time control for robotics applications. More DNN-based artificial intelligence (AI) workloads are expected to be used in future autonomous vehicles.\\
\subsubsection{End-to-End Deep Learning for Autonomous Vehicles}
\begin{figure}[H]
\includegraphics[width=10cm,height=15cm]{media/CNN_archo}
\centering
\caption{ CNN architecture. The network has about 27 million connections and 250 thousand parameters.}
\end{figure}
A traditional strategy to solving the problem of autonomous driving has been to decompose the problem into many sub-problems, such as lane marker detection, path planning, and low-level control, which together comprise a processing pipeline. Recently, researchers have begun to investigate another strategy that drastically streamlines the typical control pipeline by directly producing control outputs from sensor inputs using deep neural networks.In the late 1980s, a small 3-layer fully connected neural network was used to demonstrate the use of neural networks for end-to-end control of autonomous vehicles. Later, in the early 2000s, a DARPA Autonomous Vehicle (DAVE) project  used a 6-layer convolutional neural network (CNN), and most recently, NVIDIA's DAVE-2 project used a 9-layer CNN. All of these experiments use neural network models instead of the traditional hand-written rules and intermediary procedures that are used in traditional robotics control. The neural network models use the raw picture pixels as input and directly output steering control signals. According to NVIDIA's most recent endeavor, its trained CNN drives their modified cars on public roads without any assistance from a human.\\
\subsection{Adversarial Attacks}
\begin{figure}[H]
\includegraphics[width=13cm,height=8cm]{media/adversarial_image.png}
\centering
\caption{An adversarial input, overlaid on a typical image, can cause a classifier to miscategorize a panda as a gibbon.}
\end{figure}
With the rapid advancement of deep learning (DL) and artificial intelligence (AI) approaches, it is essential to guarantee the security and resilience of the implemented algorithms. The security weakness of DL algorithms to hostile samples has recently gained widespread recognition. The fake samples, however they appear harmless to humans, might cause numerous errors in DL models. Adversarial attacks are successfully used in real-world situations to further illustrate its applicability.\\
%
As a result, adversarial attack and defense approaches have gained popularity as a study area in recent years among both the machine learning and security communities. The theoretical underpinnings, methods, and applications of adversarial attack strategies are originally introduced in this study. We then go over a few research projects on defensive strategies that span the field's broad horizon. The discussion of a few unresolved issues and difficulties that remain will hopefully spur additional study in this crucial field.\\
\begin{figure}[H]
\includegraphics[width=14cm,height=8cm]{media/blackandwhite.png}
\centering
\caption{Overview of white and black box attack}
\end{figure}
\subsubsection{White Box Attacks}
The adversary in a white-box attack is fully aware of the target model. The learnt weights and tuning parameters for the model are among the information the opponent is aware of. In other circumstances, labeled training data is also accessible. With this knowledge, the attacker will often model the distribution using weights and generate perturbed inputs in order to breach the bounds.\\
\begin{figure}[H]
\includegraphics[width=14cm,height=8cm]{media/whiteboxattack.png}
\centering
\caption{White-box attack scenario.}
\end{figure}
%
White-box attack techniques often presuppose that the attacker is fully aware of the architecture and characteristics of the target model. The perturbation is frequently limited within a specific allowable perturbation budget or its Lp is smaller than a specific magnitude, i.e. $  \left \| v \right \|_{p} \leq e $
 to make the adversarial perturbation undetectable. Under such restrictions, the majority of adversarial assaults now in use aim to maximize a certain loss $L(x+v,y)$, for which the CE loss is frequently used.\\
\subsubsection{Black Box Attacks}
A black-box attack is one in which the attacker has little understanding of the model and, occasionally, no labeled data. Black-box constraints are frequently attacked by querying the model on inputs, checking the labels, or looking at the confidence levels. Black-box assaults are more practical even though they restrict the attacker's options. These attacks are typically conducted on fully developed and deployed systems. In the actual world, attacks with the purpose of getting around a system, taking it offline, or jeopardizing its integrity occur.\\
%
In this type of attack, the opponent and challenger can both be taken into account. A challenger is a party that develops a model and deploys it, whereas an adversary is an attacker who seeks to compromise the system in order to achieve a specific objective. In this context, a variety of capability configurations that mimic real-world behavior are feasible. As an illustration, configuration adjustments to adversarial user objectives might be made if a robber fleeing the crime scene tries to trick the surveillance system into tagging a different car number or if a criminal targets the prey's car number getting tagged. However, in this instance, a different entity owns the system that is being attacked. The monitoring system was not created with the robber or the criminal in mind.\\
%
The adversary's restricted skills are simply a mirror of actual events. Therefore, research in this area is more useful.\\
\subsection{Real World-Application}
Creating data examples that force the machine learning model to make a false prediction or collapse is the aim of adversarial machine learning. These situations usually make use of the numerical representations of the data by being virtually undetectable to humans and not raising any red flags.
Adversarial instances diverge from the statistical features that are normally used to train machine learning models, which leads to poor performance.\\
%
One of the most well-known applications of an adversarial strategy is a number of successful tests aiming at self-driving car identification models. For instance, researchers were able to completely fool the traffic sign recognition system into thinking that the stop sign reflected a speed limit using simple physical tactics.\\
Here are the reasons why adversarial attacks are important:-
\begin{enumerate}
    \item To design robust models: \\
    A product must undergo a variety of input testing kinds during a large-scale development before being judged safe for industrial usage. Either the system will know what to do with the input, or it won't.\\
    %
    However, machine learning has largely avoided this. An enormous number of out-of-sample possibilities, where the model is not expected to perform by default, was one of the key causes of this. Since there is no way to assess the possible utility of input before the model, the openness of the input channels results in feeding the malevolent example.\\
    %
    Even more, assistance will be provided in dealing with unanticipated out-of-sample inputs or attacks on the system by the availability of adversarial samples and the inclusion of technologies capable of neutralizing them
    \item  Acknowledging the Impacts:\\
    Opportunities to introduce a new algorithm into some decision-making processes continue to expand and become more ambitious as the artificial intelligence field develops into one that is incredibly significant. Meanwhile, the development pipeline still frequently follows the "collect data-train-test-deploy" pattern.\\
    %
    Contradictory machine learning demonstrates why this fails in both large-scale and commercial applications. There are other horrifying examples of how to trick automatic car behaviour with only a few carefully chosen stickers on a traffic sign or how to pretend to be a doctor by using an image that appears to be normal. Because of adversarial machine learning, we are forced to consider the decisions we are making, their significance, and if we have enough resources to defend them.\\
    \item Earning customer confidence :\\
    The overall field of artificial intelligence has been deficient in a sense of security and stability. Typically, the typical client is unaware of how precise and secure the AI system is and why it makes certain conclusions. Those potential customers who place a high value on security and performance stability may be won over by demonstrating the system's stability.\\
    \item Need for White Hats: \\
    It has long been necessary to test the security of any digital system in order to make sure that it is adequately protected against potential assaults. As a result, numerous specialized methods for producing adversarial instances already exist, with some of them even becoming well-known tools, such as the Fast Sign Gradient Method developed by Ian Goodfellow and others.\\
    %
    Because there is such a wide range of adversarial tactics that could be used, new countermeasures are always being developed. The models' vulnerability becomes apparent as a result. In locations with greater security demands, it can be crucial to stay one step ahead of the breaching risks. As a result, investigating model flaws and how to fix them could be added as a new testing stage in the model preparation process.\\
\end{enumerate}
\subsection{Organization of the Project Report}
The project report is organized as given below: \\
%
In Chapter (2), we discuss the problem statement and our solution to the problem. The same chapter also deals with the other existing technology. The Chapter that follows i.e. chapter (3) consists of the details on the literature survey of the papers to the problem statement and the proposed solution. In Chapter (4), we present the System Outline and other systems in the form of a Data flow diagram and a sequence diagram. The next chapter, chapter (5) gives the requirements and details about the implementation of the proposed system. Chapter 6 deals with the testing of the product and its desired results. In Chapter (7), we discuss the influencing parameters and their effect on the system. The same chapter deals with establishing the optimal parameters for the system. Chapter (8) concludes the paper along with a mention of the Future Enhancements. Chapter (9) details the references made during the development of the system. The other supporting information and the source code are gathered in the Appendix. \\
\section{Problem Statement and Proposed Solution}
\subsection{Problem Statement}
To demonstrate popular adversarial attacks on an end to end regression-based autonomous driving model\\
\subsection{Existing Systems:}
\subsubsection{Adversarial Attacks on Classification Models}
Adversarial attacks against classification tasks have now been extensively explored in the last decade, however, we find that comparatively few works focus on regression problems.\\
%
For classification problems, the assault is effective if the prediction departs from the ground truth. For regression tasks, an acceptable prediction could be within a range. For example, the estimated property price can vary within a fair range. Using the real number as the ground truth, we can utilize Root Mean Square Error (RMSE) to assess the effectiveness of attacks. An effective attack should result in a larger RMSE loss than random noises. An adversarial attack on a classification model builds an imperceptible adversarial perturbation to form an adversarial example $x_{0} = x + e$ and have the target model classify $x_{0}$ as $c_{0}$ that is distinct from c given a target model f and an original image x with its class c.\\
\subsection{Proposed Solution}
Current research on autonomous driving, which is a regression task, focuses on asynchronous offline attacks. The driving record is divided into static images and accompanying steering angles, and the attack is applied to each static image to calculate the total success rate. However, many road accidents are the result of tiny errors made at a key juncture. As a result, some stealth attacks with low overall success rates may nonetheless be dangerous. On the other hand, driving models, like human drivers, can react to adversarial attacks; some attacks may be negated by model reactions if applied synchronously. To explore those stealth attacks and the responses of driving models to those attacks, we would like to conduct online attacks, which means the attack will be carried out while the vehicle is travelling. Online attacks on real-world autonomous driving systems are dangerous. As a result, our adversarial driving system makes use of a self-driving simulator. For example, the fast gradient sign technique (FGSM), which linearizes the cost function around the current value of and produces an ideal max-norm restricted perturbation, is a previous offline assault that typically relies on the ground truth. The cost $J(x, y)$ cannot be determined for an autonomous navigation system since there is no ground truth for the steering command. Even an experienced human driver can perform various tasks in the same environment. Driving safely is a common driving instruction. As a result, we must decide on an appropriate ground truth for our assaults. \\
%
Our attack strategies rely on a number of presumptions:\\
%
We launch our attacks without any pre-labelled ground truth online, we can accept the driving model's output as the actual situation because it is precise enough and we need not attack if the model is flawed in and of itself. The driving task should be failed by the model itself. If the deviation under attack is greater than the divergence under random sounds, then attack is said to be successful.By making these assumptions, we are able to create two distinct attack types that may be used to influence the behaviour of the entire autonomous driving system.
%
\subsubsection{Fast Gradient Sign Method on Regression Models}
In this white-box approach, the perturbation is determined at each timestep. This assault has the ability to steer the car in the desired direction (either left or right). With input of x and prediction of f, a neural network is indicated by the notation $f(x)$. Regression model attacks may be thought of as binary targeted attacks. We have two options: we can either make the projection bigger or smaller.\\
In $\eta = sign(\bigtriangledown x\pm f(x))$
We directly linearize the model's output rather than the cost function in order to influence the behavior of the driving model. The output will grow when f(x) is linearized, but the output will decrease when $f(x)$ is linearized.Our steering command has a left to right range of -1 to 1. The expected steering command will grow if we linearize the function $f(x)$, pushing the car to the right as a result of the attack. Likewise, by linearizing $f(x)$, we may target the vehicle's left side.
\subsubsection{ Universal Adversarial Perturbation on Regression Models}
White-box attack of this kind determines a universal perturbation for each timestep. The two procedures that make up the attack are learning and executing. During the learning process, we will create the universal perturbation for each frame by linearizing the model's output, and then identify the minimal perturbation that shifts the prediction's sign in the desired direction.
\begin{flalign}
\bigtriangledown \eta \leftarrow  \frac{_{\bigtriangledown_{x}(\pm f(x))}}{\left \| \omega  \right \|_{2}}
\end{flalign}
To make sure the requirement 
$\left \| \eta _{0} \right \|_{p} \leq \varepsilon $
is met, we then project the total perturbation onto the lp ball with a radius of nI and a center at 0.
\begin{flalign}
P_{\rho ,\varepsilon}(\eta ) = arg min\left \| \eta - \eta {_{}}' \right \|_{2} subject\left \| \eta {}' \right \|_{p} \leq \varepsilon 
\end{flalign}\\
\section{Literature Survey}
\begin{enumerate}
    \item In a work by Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow and Rob Fergus, intriguing properties in neural networks are being found where Deep neural network models can be deliberately misclassified by applying a certain hardly perceptible perturbation. In addition, the true form of these perturbations is not a random artifact of learning, similar perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.
    \item In a research by Goodfellow IJ, Shlens J, Szegedy C, a method that adds perturbations from different models to training data is being undertaken. Ensemble Adversarial Training produces models on ImageNet that are more resistant to blackbox attacks. A more sophisticated black-box attack, however, might greatly increase transferability and decrease the accuracy of neural models.
    \item In a work by Tramèr F, Kurakin A, Papernot N, Goodfellow I, Boneh D, McDaniel P, a degenerate global minimum is reached using an adversarial training method where minor curvature artifacts close to the data points obscure a linear approximation of the loss. In order to guard against powerful perturbations, the model instead learns to generate mild ones.Because of this, we discover that adversarial training is still susceptible to black-box assaults, in which we transfer perturbations computed on undefended models, as well as to a potent new single-step attack that exploits the input data's non-smooth surroundings by taking a little random step. Ensemble Adversarial Training, a method that adds perturbations obtained from other models to training data is being implemented. Ensemble Adversarial Training produces models on ImageNet that are more resistant to blackbox attacks.
    \item In a work by Kannan H, Kurakin A, Goodfellow I, ImageNet is used  to achieve the most advanced kind of adversarial training at an unprecedented scale.The next step is the introduction of improved defenses using a strategy called "logit pairing," which encourages logits for pairs of samples to be similar. Finally, adversarial logit pairing, with an increase in accuracy from 1.5 percent to 27.9 percent, achieves the most advanced resistance on Imagenet against PGD white box attacks, are demonstrated.
    \item In a work by Lee H, Han S, Lee J, a novel method for leveraging a generative adversarial network to strengthen neural networks against malicious samples is being provided. Both classifier and generator networks are alternatively trained.This is the first technique that enhances supervised learning using GAN.
    \item In a work by Chuan Guo, Mayank Rana, Moustapha Cisse & Laurens van der Maaten, The study looks into methods that protect image-classification systems from adversarial-example attacks by modifying the inputs prior to feeding them to the system. To be more precise, transforming images using techniques like bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding them to neural network classifiers is being looked at.
    \item In a work by Xu W, Evans D, Qi Y By identifying hostile examples, a novel technique called "feature squeezing" that can be utilized to strengthen DNN models is being suggested. The search space is decreased via feature squeezing made accessible to a foe by combining samples that match a large number of various feature vectors from the initial space into a single sample. By contrasting a DNN model's forecast with the original data, squeezed inputs, feature squeezing recognises inputs with adversarial examples with low false positives and high accuracy.
    \item In the work by Samangouei P, Kabkab M, Chellappa R, Defense-GAN, a fresh framework that protects deep neural networks from such assaults by using the expressive power of generative models is suggested. To simulate the distribution of unaltered images, defense-GAN is trained. It locates an output that is almost identical to a given image at inference time but does not include the adversarial alterations. The classifier is then fed this output. Any classification model can be utilized, and neither the classifier structure nor the training process are changed. Defense-GAN outperforms conventional defense tactics and is consistently effective against various attack vectors.
    \item In the proposition by Meng D, Chen H, MagNet, a system for protecting neural network classifiers from hostile samples is being suggested. Both the protected classifier and the method for creating adversarial instances are not changed by MagNet. They generalize well since they don't presume any particular method for producing adversarial examples. The reformer network successfully classifies adversarial cases with minimal disruption by moving adversarial examples toward the manifold of normal examples.
    \item In a work by Carlini N, Wagner D, Recent proposals for a defense against hostile instances include MagNet and "Efficient Defenses...". With only a little increase in distortion,hostile cases that overcome these protections can be built.
\end{enumerate}

\section{Architecture and System Design}
\subsection{System Overview}
The simulator, the server, and the front-end web app built using HTML and vanilla javascript are the three main parts of our adversarial driving system. 
\subsection{Model Architecture Design}
The model that NVIDIA utilized for its end-to-end self-driving test served as the foundation for the network's design. It is therefore a good fit for the project.\\
Deep convolutional networks are effective for supervised picture classification and regression. Due to the NVIDIA model's extensive documentation, I was able to concentrate on how to modify the training photos to yield the best results while also making tweaks to the model to prevent overfitting and introduce non-linearity to enhance prediction.\\
The Model therefore looks like this : \\
Image normalization\\
Convolution: 5x5, filter: 24, strides: 2x2, activation: ELU\\
Convolution: 5x5, filter: 36, strides: 2x2, activation: ELU\\
Convolution: 5x5, filter: 48, strides: 2x2, activation: ELU\\
Convolution: 3x3, filter: 64, strides: 1x1, activation: ELU\\
Convolution: 3x3, filter: 64, strides: 1x1, activation: ELU\\
Drop out (0.5)\\
Fully connected: neurons: 100, activation: ELU\\
Fully connected: neurons: 50, activation: ELU\\
Fully connected: neurons: 10, activation: ELU\\
Fully connected: neurons: 1 (output)\\
According to the NVIDIA model, the fully connected layer is used to anticipate the steering angle while the convolution layers are designed to manage feature engineering. However, it is unclear where to draw such a clear demarcation, as noted in the NVIDIA document. Overall, the model is really useful for duplicating the provided steering behavior.\\
The model structure output from Keras shown below provides further information on the shapes and amount of parameters
\begin{figure}[H]
\includegraphics[width=14cm,height=8cm]{media/layers_screenshot.png}
\centering
\caption{CNN architecture of the self-driving autonomous vehicle}
\end{figure}
\subsection{System Architecture components}
The simulator : Unity3D, a cross-platform game engine developed by Unity Technologies, is used to create three-dimensional (3D) and two-dimensional (2D) games, as well as interactive simulations and other experiences, in the self-driving simulator. Outside of video games, the engine has been used in film, automotive, architecture, engineering, construction, and the United States Armed Forces. Once connected, the simulator publishes the image of each frame and accepts steering commands from the server.
\subsubsection{The backend server}
The WebSocket backend is built on flask, a server-side Python framework that is lightweight and modular. It accepts simulator connections and returns the control command. Furthermore, it will publish to the front end the noise generated per frame and the generated adversarial images, as well as receive attack commands from the web browser.
\subsubsection{The web app}
The front end is a website built with HTML, CSS, and plain JavaScript in which the attacker may select several types of attacks (currently implemented FGSM-r both left and right, Universal Perturbation attack both left and right) and monitor the simulator's status (check the actual image frame, the noise generated, the perturbed image, actual steering angles and predicted steering angles).
\begin{figure}[H]
\includegraphics[width=14cm,height=8cm]{media/layers_screenshot.png}
\centering
\caption{CNN architecture of the self-driving autonomous vehicle}
\end{figure}
\subsection{System Architecture Block diagram}
\begin{figure}[H]
\includegraphics[width=14cm,height=8cm]{media/system_design.png}
\centering
\caption{System Design}
\end{figure}



\section{Implementation}
\subsection{Software Requirements}
\begin{enumerate}
    \item \textbf{Udacity's Self-Driving Car Simulator}
    A self-driving car simulator has been used for the project .  It was developed by Udacity for a nanodegree program. There are multiple maps present in the simulator. We are demonstrating our project on one of the maps. Simulator makes use of Unity.\\
    Unity is a cross-platform game engine developed by Unity Technologies, first announced and released in June 2005 at Apple Worldwide Developers. 
    \item \textbf{Keras} 
    A Python interface for artificial neural networks is provided by the open-source software package known as Keras. The TensorFlow library interface is provided by Keras.\\
    A Python interface for artificial neural networks is provided by the open-source software package known as Keras. The TensorFlow library interface is provided by Keras.\\
    One and only TensorFlow is supported as of version 2.4. Its user-friendliness, modularity, and extensibility are its main design goals as it aims to facilitate quick experimentation with deep neural networks. It was created as a component of the research project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System), and François Chollet, a Google engineer, is its principal author and maintainer. The deep neural network model for Xception was also created by Chollet.
    \item \textbf{Socket.IO}
    An event-driven library for real-time web applications is Socket.IO. It makes it possible for web clients and servers to communicate in real time and in both directions. It consists of a server-side library for Node.js and a client-side library that runs in the browser. The APIs of the two components are remarkably similar.\\
    While offering the similar interface, Socket.IO primarily uses the WebSocket protocol, with polling as a backup method. Although it can be used as a simple WebSockets wrapper, it offers a wide range of additional functionalities, such as broadcasting to many sockets, storing client-specific data, and asynchronous I/O.
    \item \textbf{Flask}
     Developed in Python, Flask is a microweb framework. Due to the fact that it doesn't require any specific tools or libraries, it is categorised as a microframework. [2] It lacks any components where pre-existing third-party libraries already provide common functions, such as a database abstraction layer, form validation, or other components. However, Flask allows for extensions that may be used to add application functionalities just like they were built into the core of Flask. There are extensions for object-relational mappers, form validation, upload handling, several open authentication protocols, and a number of utilities associated with popular frameworks. \\
     The Flask framework is used by applications like Pinterest and LinkedIn.
\end{enumerate}

\subsection{ Implentation Details.}
\subsubsection{Install Simulator}
\begin{enumerate}
    \item Download the simulator into your local machine , install unity into your machine to load the asset files from the simulator.
    \item After installing unity load the download folder inside unity application. 
    \item Load up scenes by going to Project tab in the bottom left, and navigating to the folder Assets/SelfDrivingCar/Scenes.
    \item View Scripts. Scripts are what make all the different mechanics of the simulator work and they are located in two different directories, the first is Assets/SelfDrivingCar/Scripts which mostly relate to the UI and socket connections. The second directory for scripts is Assets/Standard Assets/Vehicle/Car/Scripts and they control all the different interactions with the car.
\end{enumerate}

\subsubsection{Data Collection}
\begin{enumerate}
    \item After opening the simulator, record the track. Drive the car manually. 
    \begin{figure}[H]
    \includegraphics[width=16cm,height=9cm]{media/capturing.jpeg}
    \centering
    \caption{Driving manually to capture parameters}
    \end{figure}
    \item This captures data from front, left, right cameras, with their corresponding steering angle. 
    \begin{figure}[H]
    \includegraphics[width=16cm,height=9cm]{media/Data collection system.png}
    \centering
    \caption{Data Collection System}
    \end{figure}
    \item Data gets stored in an excel format
    \begin{figure}[H]
    \includegraphics[width=16cm,height=9cm]{media/excel.jpeg}
    \centering
    \caption{Collected data in an excel format to train the model}
    \end{figure}
    \item After recording for 2 hours. The obtained data is in this folder structure
    \begin{figure}[H]
    \includegraphics[width=16cm,height=9cm]{media/folder.jpeg}
    \centering
    \end{figure}
\end{enumerate}

\subsubsection{File Structure}
This is the folder structure of our project
\begin{figure}[H]
\includegraphics[width=16cm,height=9cm]{media/folder_structure.png}
\centering
\end{figure}
Client directory:
\begin{figure}[H]
\includegraphics[width=16cm,height=9cm]{media/client_folder_structure.png}
\centering
\end{figure}


















\section{Experimentation and Results}
\begin{enumerate}
    \item \textbf{Random noise attack}
    \begin{figure}[H]
    \includegraphics[width=16cm,height=9cm]{media/random_noise.png}
    \centering
    \caption{Image showing adding perturbation to an input image from the front angle}
    \end{figure}

    \begin{figure}[H]
    \includegraphics[width=16cm,height=9cm]{media/random_noise_steeringangle.png}
    \centering
    \caption{The random attack where random noise is added to the input image.
    There is no deviation in the steering angle when random noise is added}
    \end{figure}
    
    \item \textbf{FGSMr : Left}
    \begin{figure}[H]
    \includegraphics[width=16cm,height=9cm]{media/fgsn-left-purt.png}
    \centering
    \end{figure}
    
    \begin{figure}[H]
    \includegraphics[width=16cm,height=9cm]{media/fgsm-left-steering.png}
    \centering
    \caption{The image shows the FGSMr : Left where the steering angle is pulled to left causing a collision.}
    \end{figure}
    
    \item \textbf{FGSMr : Right}
    \begin{figure}[H]
    \includegraphics[width=16cm,height=9cm]{media/fgsm-right-purt.png}
    \centering
    \end{figure}
    
    \begin{figure}[H]
    \includegraphics[width=16cm,height=9cm]{media/fgsm-right-steering.png}
    \centering
    \caption{The image shows the FGSMr : Right where the steering angle is pulled to right causing a collision.}
    \end{figure}
    
    \item \textbf{UAPr No Left}
    
    \begin{figure}[H]
    \includegraphics[width=16cm,height=9cm]{media/uap-left-pirt.png}
    \centering
    \end{figure}
    
    \begin{figure}[H]
    \includegraphics[width=16cm,height=9cm]{media/uap-left-steering.png}
    \centering
    \caption{The UAPr No Left is a stealth attack. The attack is slow and the steering angle with attack  is being avoided to go left. This attack may cause collision with other cars.}
    \end{figure}
    
    \item \textbf{UAPr No Right}
    \begin{figure}[H]
    \includegraphics[width=16cm,height=9cm]{media/uap-right-purt.png}
    \centering
    \end{figure}
    

    \begin{figure}[H]
    \includegraphics[width=16cm,height=9cm]{media/uap-right-steering.png}
    \centering
    \caption{The UAPr No Right is a stealth attack. The attack is slow and the steering angle with attack  is being avoided to go right. This attack may cause collisions with other cars.}
    \end{figure}

\end{enumerate}

The NVIDIA end-to-end self-driving model is subjected to a number of threats . The steering command is in the -1 to 1 range (from left to right). Below table shows the average absolute and relative steering angle deviations for attacks under 1000 attacks

\begin{figure}[H]
\includegraphics[width=16cm,height=9cm]{media/table.png}
\centering
\end{figure}

\textbf{Attacks conducted}\\
\begin{enumerate}
    \item Fast Gradient Sign Method is a strong attack compared to other attacks.In a few seconds after being attacked, the car will go off the road. Different assault directions have similar absolute variances. FGSM right will avoid the car from going right and FGSM left does the opposite.
    \item The absolute deviation is substantially smaller than the FGSMr and bigger than random sounds, showing that it is effective. The attack is covert because the steering command's tiny deviation makes the vehicle difficult to control. At some crucial points, this might result in incidents. Additionally, because the same perturbation is used for all frames, it is faster than the FGSMr.
\end{enumerate}

Finally, we design a powerful attack (FGSMr) and a stealth assault (UAPr). While a covert attack may result in mishaps at specific crucial locations, a strong attack will deviate the vehicle in a matter of seconds. The end-to-end driving model is hence susceptible to hostile attacks.\\


\textbf{\LARGE{Conclusion and Future Work}}\\
In this study, we develop two end-to-end targeted white-box assaults against autonomous driving systems. By including perturbations to the input image, it is possible to change the behavior of the driving model. Our research shows that adversarial attacks can also be used against the autonomous driving system, which is a regression problem.\\
Additionally, additional research might be done to look into how black-box attacks affect end-to-end autonomous driving systems. Modular systems that receive input from a variety of sensors may potentially be vulnerable to hostile attacks. Concerns regarding the use of end-to-end models in safety-critical systems may be raised by this research.\\



\section{References}

To include References the right way (using Biblatex), search for a Bibtex generator on Google, and feed its output to a seperate Bibtex (Bib) file. Then we simply reference it in our main file and you will get the citations neatly ordered. If you want a quicker (improper hack), then just paste the results of the citation generator. \\
$[1]$ Figure 2f from: Irimia R, Gottschling M (2016) Taxonomic revision of Rochefortia Sw. (Ehretiaceae, Boraginales). Biodiversity Data Journal 4:e7720 
\\
$[2]$ BRANDON, J. 2007. Similarity of temporal query logs. Doctoral dissertation. University of California, Los Angeles. \\

Please note that the citation formats for different documents (articles, reports, dissertations,papers, conference proceedings) and organisations (ACM, IEEE) are different. Use a consistent style.
\end{document}
